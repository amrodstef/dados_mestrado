---
title: "Coleta ((o))eco"
author: "Andre M. Rodeguero Stefanuto"
date: "2023-12-08"
output: pdf_document
--- 

# PRIMEIRA ETAPA

```{r}
library(rvest)
library(stringr)
library(tidyverse)
library(httr)
library(RCurl)
library(xml2)
```


## PRIMEIRA ETAPA: COLETA DE TEXTOS

O primeiro passo da coleta de dados no ((o))eco utilizando o __web scraping__ é obter todos os links das matérias através do buscador do site. Para tanto, preciso (1) definir a lista de palavras-chave da busca; e (2) criar um __data frame__ no qual armazenarei os links obtidos e as palavras chave que utilizei para obtê-los.

```{r}
keywords_carb <- c("mercado+de+carbono", "créditos+de+carbono", "compensações+de+carbono", "comércio+de+carbono")

links_carb <- data.frame(link = character(), keyword = character(), stringsAsFactors = FALSE)
```


Isto feito, podemos realizar a coleta desses links. Ao longo do código, haverá comentários sobre partes específicas do código, além das descrições dada entre cada uma das etapas do processo.

```{r}
# Criando uma lista vazia para guardar os links; abaixo, ela será armazenada em um data frame
keylinklist_carb <- list()

#Iniciamos o for() para coleta dos links resultantes em cada uma das páginas de resultados;
for (keyword in keywords_carb) {
        encoded_keyword <- URLencode(keyword)
        keyword_links <- data.frame(link = character(), keyword = character(), stringsAsFactors = FALSE)
        
        num_page <- 1
        while (TRUE) {
                url <- paste0("https://oeco.org.br/page/1/?s=", encoded_keyword, "&paged=", num_page)
                response <- GET(url)
                page_content <- content(response, as = "text")
                page <- read_html(page_content)
                links <- page %>%
                        html_nodes("article h2 a") %>% #id. de links na página de busca
                        html_attr("href") #id do link de referência e não o título exibido
                
                if (length(links) == 0) {
                        break  #Se não é mais links para buscar, podemos encerrar o loop
                }
                
                keyword_links <- rbind(keyword_links, data.frame(link = links, keyword = keyword, stringsAsFactors = FALSE))
                
                num_page <- num_page + 1
        }
        
        # Armazena os links obtidos e suas respectivas palavras-chave em uma lista contendo os data frames das diferentes palavras
        keylinklist_carb <- c(keylinklist_carb, list(keyword_links))
}
```

```{r}
# Combina os diferentes data frames em um só.
links_carb <- do.call(rbind, keylinklist_carb)
```


O objeto "links_carb" contém todos os links obtidos a partir das buscas com cada uma das palavras-chave em sua coluna "link" e a palavra chave utilizada para obtenção daquele link na coluna "keyword". Ressalto que uma matéria pode ser encontrada a partir da busca com diferentes palavras chave. Para evitar a repetição de matérias nesta primeira busca, realizaremos a unificação delas. Cada matéria será observada uma primeira vez e todas as outras vezes em que aparecer, será removida. Sendo assim, a primeira palavra-chave não conterá matérias removidas pois apresentará somente matérias inéditas. 

Devemos realizar a unificação das matérias encontradas com cada uma das palavras chave aplicadas, mantendo "mangue" ao fim delas.

```{r}
un_links_carb <- data.frame(link = links_carb[!duplicated(links_carb$link), ])
names(un_links_carb) <- c("link", "keyword")
```


Já com todas as matérias coletadas e unificadas, não há mais repetições dos links. Podemos iniciar o processo de extração das informações de cada um desses links. Vamos, então, preparar cada uma das funções necessárias para extração de cada dado desejado:

```{r}
#Aqui, criamos a função responsável pela extração do título das matérias
extract_title_eco <- function(url){
        page <- read_html(url)
        title <- page %>% 
                html_node("h1") %>%
                html_text() %>%
                trimws()
        return(title)
}

#Se houver necessidade de testar a função:
#url <- un_links_eco_carb$link[10]
#extract_title_eco(url)
```

Em seguida, podemos desenvolver a função para extração das datas:

```{r}
extract_date_eco <- function(url){
        page <- read_html(url)
        metadata <- page %>%
                html_nodes("meta") %>%
                html_attr("content")
        date <- page %>% 
                html_node('meta[property="article:published_time"]') %>%
                html_attr("content") %>% 
                trimws()
        date <- str_split(date, "T")
        date <- date[[1]][1] 
        date <- as.Date(date)
        return(date)
}

#Se houver necessidade de testar a função:
#url <- un_links_eco_mang$link[15]
#extract_date_eco(url)
```

Com as funções corretamente criadas e funcionando em links individuais, é hora de colocá-las para rodar dentro de todo o conjunto de dados. Para tanto, criamos um __data frame__ vazio que receberá os dados conforme forem sendo coletados, um a um, dentro do for().

```{r}
oeco_data_carb <- data.frame(title = character(),
                        date = character(),
                        link = character(),
                        stringsAsFactors = FALSE)
```

Em seguida, rodamos cada uma das funções de extração criadas acima dentro de um for, seguindo o critério de recorte temporal do trabalho:

```{r}
for (url in un_links_carb$link[1:length(un_links_carb$link)]){
       tryCatch({
         date <- extract_date_eco(url)
        if (date >= as.Date("2022-05-19") && date <= as.Date("2023-11-04") && !is.na(date)) {
                full_text <- extract_text_eco(url)
              
                oeco_data_carb <- rbind(oeco_data_carb, data.frame(title = title,
                                                              date = date,
                                                              link = url,
                                                              stringsAsFactors = FALSE))
        }
    }, error = function(e) {
        cat("Error processing URL:", url, "\n")
        cat("Error message:", conditionMessage(e), "\n")
    })
}
```

Aqui, selecionamos as publicações coletadas, e a cada uma delas atribuímos um código de identificação.

```{r}
View(oeco_data_carb)
reco_carb_data <- oeco_data_carb %>% 
        arrange(date)

reco_carb_data <- reco_carb_data %>% 
                        mutate(code = paste0(rep("eco_", length(oeco_data_carb$title)),
                                sprintf("%03d", 1:length(oeco_data_carb$title))))

write.csv(reco_carb_data, "oeco_data_carb_reord.csv")
```
